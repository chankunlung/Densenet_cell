{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T01:45:47.667764Z",
     "start_time": "2018-03-13T01:45:47.145661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utf-8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import imp\n",
    "import os\n",
    "import sys\n",
    "print(sys.getdefaultencoding())\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T01:45:48.186029Z",
     "start_time": "2018-03-13T01:45:47.669364Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "import os\n",
    "import os.path as op\n",
    "import cv2\n",
    "from collections import namedtuple\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, data_dir, size=(312, 312)):\n",
    "        self.data_dir = data_dir\n",
    "        self.__find_images()\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.size = size\n",
    "        sample = cv2.imread(self.images[0])\n",
    "        self.img_size = (sample.shape[0], sample.shape[1])\n",
    "        self.__compute_offsets()\n",
    "        \n",
    "        self.batch_num = len(self.images)\n",
    "        \n",
    "    def __compute_offsets(self):\n",
    "        self.row_nums = int(np.ceil(self.img_size[0] / float(self.size[0])))\n",
    "        self.col_nums = int(np.ceil(self.img_size[1] / float(self.size[1])))\n",
    "        print('rows', self.row_nums)\n",
    "        print('cols', self.col_nums)\n",
    "        \n",
    "        offset_w = int((self.col_nums * self.size[1] - self.img_size[1]) / (2 * (self.col_nums - 1)))\n",
    "        self.margin_w = int(offset_w / 3)\n",
    "        \n",
    "        offset_h = int((self.row_nums * self.size[0] - self.img_size[0]) / (2 * (self.row_nums - 1)))\n",
    "        self.margin_h = int(offset_h / 3)\n",
    "        print('offset w', offset_w)\n",
    "        print('offset h', offset_h)\n",
    "    \n",
    "        self.offsets_w = [i * (self.size[1] - offset_w) for i in range(self.col_nums)]\n",
    "        self.offsets_w[-1] = self.img_size[1] - self.size[1]\n",
    "        \n",
    "        self.offsets_h = [i * (self.size[0] - offset_h) for i in range(self.row_nums)]\n",
    "        self.offsets_h[-1] = self.img_size[0] - self.size[0]\n",
    "        \n",
    "    def __split_sample(self, sample):\n",
    "        splitted = []\n",
    "        \n",
    "        for offset_h in self.offsets_h:\n",
    "            for offset_w in self.offsets_w:\n",
    "                h1, h2 = offset_h, offset_h + self.size[0]\n",
    "                w1, w2 = offset_w, offset_w + self.size[1]\n",
    "                splitted.append(sample[h1: h2, w1: w2])\n",
    "        return splitted\n",
    "    \n",
    "    def __compute_merge_offsets(self, offset_h, offset_w, pred):\n",
    "        if offset_h == 0:\n",
    "            h1 = 0\n",
    "            h2 = self.size[0] - self.margin_h\n",
    "            pred = pred[:, :pred.shape[1]-self.margin_h, :]\n",
    "        elif offset_h == self.offsets_h[-1]:\n",
    "            h1 = offset_h + self.margin_h\n",
    "            h2 = offset_h + self.size[0]\n",
    "            pred = pred[:, self.margin_h:, :]\n",
    "        else:\n",
    "            h1 = offset_h + self.margin_h\n",
    "            h2 = h1 + self.size[0] - 2*self.margin_h\n",
    "            pred = pred[:, self.margin_h: pred.shape[1]-self.margin_h, :]\n",
    "            \n",
    "        if offset_w == 0:\n",
    "            w1 = 0\n",
    "            w2 = self.size[1] - self.margin_w\n",
    "            pred = pred[:, :, :pred.shape[2]-self.margin_w]\n",
    "        elif offset_w == self.offsets_w[-1]:\n",
    "            w1 = offset_w + self.margin_w\n",
    "            w2 = offset_w + self.size[1]\n",
    "            pred = pred[:, :, self.margin_w:]\n",
    "        else:\n",
    "            w1 = offset_w + self.margin_w\n",
    "            w2 = w1 + self.size[1] - 2*self.margin_w\n",
    "            pred = pred[:, :, self.margin_w:pred.shape[2]-self.margin_w]\n",
    "        return h1, h2, w1, w2, pred\n",
    "    \n",
    "    def recreate_mask(self, predicted):\n",
    "        final = np.ones((3, self.img_size[0], self.img_size[1])) * np.NINF\n",
    "        \n",
    "        for offset_h in self.offsets_h:\n",
    "            for offset_w in self.offsets_w:\n",
    "                \n",
    "                h1, h2, w1, w2, pred = self.__compute_merge_offsets(offset_h, offset_w, predicted.pop(0)[0])\n",
    "        \n",
    "                final[:, h1: h2, w1: w2] = np.maximum(pred, final[:, h1: h2, w1: w2])\n",
    "        return final\n",
    "        \n",
    "    def __find_images(self):\n",
    "        self.images = [op.join(self.data_dir, path) for path in os.listdir(self.data_dir)]\n",
    "    \n",
    "    def __load_data(self, sample):\n",
    "        inputs = [] \n",
    "        name = op.splitext(op.basename(sample))[0]\n",
    "        \n",
    "        raw = cv2.imread(sample, 0)\n",
    "        splitted = self.__split_sample(raw)\n",
    "            \n",
    "        for train_sample in splitted:\n",
    "            inputs.append(torch.from_numpy(np.expand_dims(np.expand_dims(np.array(train_sample) / 225., 0), 0)).float())\n",
    "            \n",
    "        return name, inputs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            shuffle(self.images)\n",
    "        for i in range(len(self.images)):\n",
    "            yield self.__load_data(self.images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T01:45:48.360851Z",
     "start_time": "2018-03-13T01:45:48.187560Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def center_crop(layer, max_height, max_width):\n",
    "    #https://github.com/Lasagne/Lasagne/blob/master/lasagne/layers/merge.py#L162\n",
    "    #Author does a center crop which crops both inputs (skip and upsample) to size of minimum dimension on both w/h\n",
    "    batch_size, n_channels, layer_height, layer_width = layer.size()\n",
    "    xy1 = (layer_width - max_width) // 2\n",
    "    xy2 = (layer_height - max_height) // 2\n",
    "    return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]\n",
    "\n",
    "class DenseLayer(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        \n",
    "        #author's impl - lasange 'same' pads with half \n",
    "        # filter size (rounded down) on \"both\" sides\n",
    "        self.add_module('conv', nn.Conv2d(in_channels=in_channels, \n",
    "                out_channels=growth_rate, kernel_size=3, stride=1, \n",
    "                  padding=1, bias=True))\n",
    "        \n",
    "        self.add_module('drop', nn.Dropout2d(0.2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(DenseLayer, self).forward(x)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.layers = nn.ModuleList([DenseLayer(\n",
    "            in_channels + i*growth_rate, growth_rate)\n",
    "            for i in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            new_features = []\n",
    "            #we pass all previous activations into each dense layer normally\n",
    "            #But we only store each dense layer's output in the new_features array\n",
    "            for layer in self.layers:\n",
    "                out = layer(x)\n",
    "                x = torch.cat([x, out], 1)\n",
    "                new_features.append(out)\n",
    "            return torch.cat(new_features,1)\n",
    "        else:\n",
    "            for layer in self.layers:\n",
    "                out = layer(x)\n",
    "                x = torch.cat([x, out], 1) # 1 = channel axis\n",
    "            return x \n",
    "    \n",
    "class TransitionDown(nn.Sequential):\n",
    "    def __init__(self, in_channels):\n",
    "        super(TransitionDown, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(in_channels=in_channels, \n",
    "              out_channels=in_channels, kernel_size=1, stride=1, \n",
    "                padding=0, bias=True))\n",
    "        self.add_module('drop', nn.Dropout2d(0.2))\n",
    "        self.add_module('maxpool', nn.MaxPool2d(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return super(TransitionDown, self).forward(x)\n",
    "    \n",
    "class TransitionUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TransitionUp, self).__init__()\n",
    "        self.convTrans = nn.ConvTranspose2d(in_channels=in_channels, \n",
    "               out_channels=out_channels, kernel_size=3, stride=2, \n",
    "              padding=0, bias=True) #crop = 'valid' means padding=0. Padding has reverse effect for transpose conv (reduces output size)\n",
    "        #http://lasagne.readthedocs.io/en/latest/modules/layers/conv.html#lasagne.layers.TransposedConv2DLayer\n",
    "        #self.updample2d = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        out = self.convTrans(x)\n",
    "        out = center_crop(out, skip.size(2), skip.size(3))\n",
    "        out = torch.cat([out, skip], 1)\n",
    "        return out\n",
    "    \n",
    "class Bottleneck(nn.Sequential):\n",
    "    def __init__(self, in_channels, growth_rate, n_layers):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.add_module('bottleneck', DenseBlock(in_channels, growth_rate, n_layers, upsample=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return super(Bottleneck, self).forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T01:45:48.579058Z",
     "start_time": "2018-03-13T01:45:48.362154Z"
    }
   },
   "outputs": [],
   "source": [
    "class FCDenseNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, down_blocks=(5,5,5,5,5), \n",
    "                 up_blocks=(5,5,5,5,5), bottleneck_layers=5, \n",
    "                 growth_rate=16, out_chans_first_conv=48, n_classes=3):\n",
    "        super(FCDenseNet, self).__init__()\n",
    "        self.down_blocks = down_blocks\n",
    "        self.up_blocks = up_blocks\n",
    "        \n",
    "        cur_channels_count = 0\n",
    "        skip_connection_channel_counts = []\n",
    "        \n",
    "        \n",
    "        #####################\n",
    "        # First Convolution #\n",
    "        #####################\n",
    "\n",
    "        self.add_module('firstconv', nn.Conv2d(in_channels=in_channels, \n",
    "                  out_channels=out_chans_first_conv, kernel_size=3, \n",
    "                  stride=1, padding=1, bias=True))\n",
    "        cur_channels_count = out_chans_first_conv\n",
    "        \n",
    "        \n",
    "        \n",
    "        #####################\n",
    "        # Downsampling path #\n",
    "        #####################\n",
    "        \n",
    "        self.denseBlocksDown = nn.ModuleList([])\n",
    "        self.transDownBlocks = nn.ModuleList([])\n",
    "        for i in range(len(down_blocks)):\n",
    "            self.denseBlocksDown.append(\n",
    "                DenseBlock(cur_channels_count, growth_rate, down_blocks[i]))\n",
    "            cur_channels_count += (growth_rate*down_blocks[i])\n",
    "            skip_connection_channel_counts.insert(0,cur_channels_count)\n",
    "            self.transDownBlocks.append(TransitionDown(cur_channels_count))\n",
    "            \n",
    "            \n",
    "            \n",
    "        #####################\n",
    "        #     Bottleneck    #\n",
    "        #####################\n",
    "        \n",
    "        self.add_module('bottleneck',Bottleneck(cur_channels_count, \n",
    "                                     growth_rate, bottleneck_layers))\n",
    "        prev_block_channels = growth_rate*bottleneck_layers\n",
    "        cur_channels_count += prev_block_channels \n",
    "        \n",
    "        \n",
    "        \n",
    "        #######################\n",
    "        #   Upsampling path   #\n",
    "        #######################\n",
    "\n",
    "        self.transUpBlocks = nn.ModuleList([])\n",
    "        self.denseBlocksUp = nn.ModuleList([])\n",
    "        for i in range(len(up_blocks)-1):\n",
    "            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n",
    "            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n",
    "\n",
    "            self.denseBlocksUp.append(DenseBlock(\n",
    "                cur_channels_count, growth_rate, up_blocks[i], \n",
    "                    upsample=True))\n",
    "            prev_block_channels = growth_rate*up_blocks[i]\n",
    "            cur_channels_count += prev_block_channels\n",
    "\n",
    "            \n",
    "        #One final dense block\n",
    "        self.transUpBlocks.append(TransitionUp(\n",
    "            prev_block_channels, prev_block_channels))\n",
    "        cur_channels_count = prev_block_channels + skip_connection_channel_counts[-1]\n",
    "\n",
    "        self.denseBlocksUp.append(DenseBlock(\n",
    "            cur_channels_count, growth_rate, up_blocks[-1], \n",
    "                upsample=False))\n",
    "        cur_channels_count += growth_rate*up_blocks[-1]\n",
    "\n",
    "        \n",
    "        \n",
    "        #####################\n",
    "        #      Softmax      #\n",
    "        #####################\n",
    "\n",
    "        self.finalConv = nn.Conv2d(in_channels=cur_channels_count, \n",
    "               out_channels=n_classes, kernel_size=1, stride=1, \n",
    "                   padding=0, bias=True)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(\"INPUT\",x.size())\n",
    "        out = self.firstconv(x)\n",
    "        \n",
    "        skip_connections = []\n",
    "        for i in range(len(self.down_blocks)):\n",
    "            #print(\"DBD size\",out.size())\n",
    "            out = self.denseBlocksDown[i](out)\n",
    "            skip_connections.append(out)\n",
    "            out = self.transDownBlocks[i](out)\n",
    "            \n",
    "        out = self.bottleneck(out)\n",
    "        #print (\"bnecksize\",out.size())\n",
    "        for i in range(len(self.up_blocks)):\n",
    "            skip = skip_connections.pop()\n",
    "            #print(\"DOWN_SKIP_PRE_UPSAMPLE\",out.size(),skip.size())\n",
    "            out = self.transUpBlocks[i](out, skip)\n",
    "            #print(\"DOWN_SKIP_AFT_UPSAMPLE\",out.size(),skip.size())\n",
    "            out = self.denseBlocksUp[i](out)\n",
    "            \n",
    "        out = self.finalConv(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    \n",
    "def FCDenseNet57(n_classes):\n",
    "    return FCDenseNet(in_channels=1, down_blocks=(4, 4, 4, 4, 4), \n",
    "                 up_blocks=(4, 4, 4, 4, 4), bottleneck_layers=4, \n",
    "                 growth_rate=12, out_chans_first_conv=48, n_classes=n_classes)\n",
    "\n",
    "def FCDenseNet67(n_classes):\n",
    "    return FCDenseNet(in_channels=1, down_blocks=(5, 5, 5, 5, 5), \n",
    "                 up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5, \n",
    "                 growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)\n",
    "\n",
    "def FCDenseNet103(n_classes):\n",
    "    return FCDenseNet(in_channels=1, down_blocks=(4,5,7,10,12), \n",
    "                 up_blocks=(12,10,7,5,4), bottleneck_layers=15, \n",
    "                 growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T01:45:50.459753Z",
     "start_time": "2018-03-13T01:45:48.580470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows 4\n",
      "cols 4\n",
      "offset w 37\n",
      "offset h 37\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0x80 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d48b267540e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msnapshot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCDenseNet57\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0x80 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "model_path = '/home/davince/Dropbox (OIST)/Deeplearning_system/OIST_fcdensenet_pytorch/best_model.pth'\n",
    "data_directory = '/home/davince/Dropbox (OIST)/Deeplearning_system/OIST_fcdensenet_pytorch/DeepCell/RawImages/'\n",
    "\n",
    "\n",
    "\n",
    "OUT_DIR = '/home/davince/Dropbox (OIST)/Deeplearning_system/OIST_fcdensenet_pytorch/20180516predict/'\n",
    "\n",
    "#  Training Hyperparameters\n",
    "\n",
    "N_CLASSES = 3\n",
    "\n",
    "\n",
    "path = '/home/davince/Dropbox (OIST)/Deeplearning_system/OIST_fcdensenet_pytorch/DeepCell/RawImages/'\n",
    "\n",
    "loader = DataLoader(path)\n",
    "\n",
    "snapshot = torch.load(model_path)\n",
    "\n",
    "model = FCDenseNet57(N_CLASSES)\n",
    "model.load_state_dict(snapshot['state_dict'])\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T01:45:50.514897Z",
     "start_time": "2018-03-13T01:45:50.461048Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predictions(outputs):\n",
    "    return np.argmax(outputs, 0)\n",
    "\n",
    "\n",
    "def save_predictions(out_dir, name, prediction, boundary_dir, cytoplasm_dir):   \n",
    "        \n",
    "    boundary = np.zeros(prediction.shape, dtype=np.uint8)\n",
    "    cytoplasm = np.zeros(prediction.shape, dtype=np.uint8)\n",
    "        \n",
    "    boundary[np.where(prediction == 1)] = 255\n",
    "    cytoplasm[np.where(prediction == 2)] = 255\n",
    "        \n",
    "    cv2.imwrite(op.join(boundary_dir, name+'boundary.png'), boundary)\n",
    "    cv2.imwrite(op.join(cytoplasm_dir, name+'cytoplasm.png'), cytoplasm)\n",
    "        \n",
    "        \n",
    "def get_predictions_variable(outputs):\n",
    "    bs, c, h, w = outputs.size()\n",
    "    tensor = outputs.data\n",
    "    values, indices = tensor.cpu().max(1)\n",
    "    indices = indices.view(bs, h, w)\n",
    "    return indices\n",
    "        \n",
    "\n",
    "def predict(model, loader, out_dir):\n",
    "    torch.cuda.empty_cache()\n",
    "    model.eval()\n",
    "    boundary_dir = op.join(out_dir, \"boundary\")\n",
    "    if not op.exists(boundary_dir):\n",
    "        os.makedirs(boundary_dir)\n",
    "    cytoplasm_dir = op.join(out_dir, \"cytoplasm\")\n",
    "    if not op.exists(cytoplasm_dir):\n",
    "        os.makedirs(cytoplasm_dir)\n",
    "    \n",
    "    \n",
    "    with tqdm(total=loader.batch_num) as pbar:\n",
    "        for i, data in enumerate(loader):\n",
    "            name, inputs = data\n",
    "            sample_prediction = []\n",
    "            for input in inputs:\n",
    "                outputs = model(Variable(input).cuda())\n",
    "    \n",
    "                outputs = outputs.data.cpu()\n",
    "                sample_prediction.append(outputs)\n",
    "                torch.cuda.empty_cache()\n",
    "            full_predictions = loader.recreate_mask(sample_prediction)\n",
    "                \n",
    "            save_predictions(out_dir, name, get_predictions(full_predictions), boundary_dir, cytoplasm_dir)\n",
    "            \n",
    "            pbar.update(1)\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T01:45:55.976840Z",
     "start_time": "2018-03-13T01:45:50.516120Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/148 [00:00<?, ?it/s]/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:109: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 148/148 [03:47<00:00,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 2min 7s, total: 3min 30s\n",
      "Wall time: 3min 47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict(model, loader, OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
