{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making training data sets\n",
    "This ipython notebook provides an interactive overview to the make_training_data.py script that converts annotated images into training data sets. Our starting point is that an image (phase.png and nuclear.png) and its annotation (feature_0.png - edges and feature_1.png - interior) be present in the same folder. Our ending point will be a list of all the pixels from the image that we will sample to create our training data.\n",
    "\n",
    "First lets load all of the python packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cnn_functions import format_coord as cf\n",
    "from skimage import morphology as morph\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage as sk\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "from skimage import feature\n",
    "from cnn_functions import get_image\n",
    "import glob, os, fnmatch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first have to define some parameters - the maximum number of training examples we want to sample from the image and the size of the patches sampled from our images. The window_size variables depict the number of pixels sampled from both directions to create one training image. For example, if the window_size is 30, the size of the sampled image will be 2*30+1 = 61 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_training_examples = 10000000\n",
    "window_size_x = 30\n",
    "window_size_y = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the directories where the training data is located. In this example, we will train on images of 3T3 cells. We've annotated 3 images and would like to use all 3. Note that the channel_names variable contains a list of all the channels that will be loaded. The channel names must be present in the names of the image files for the program to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc_name = '/home/davince/Dropbox (OIST)/DeepCell-master/training_data/20180316_newtrainingdata/'\n",
    "file_name_save = os.path.join('/home/davince/Dropbox (OIST)/DeepCell-master/training_data_npz/20180316/', 'cytoplasm_all_61x61.npz')\n",
    "#training_direcs = [\"set1\", \"set2\", \"set3\", \"set4\", \"set5\", \"set6\", \"set7\", \"set8\", \"set9\", \"set10\", \"set11\", \"set12\", \"set13\", \"set14\", \"set15\", \"set16\"] \n",
    "training_direcs = [\"set1\",\"set2\", \"set3\",\"set4\",\"set5\",\"set6\",\"set7\",\"set8\",\"set9\",\"set10\", \"set11\", \"set12\", \"set13\", \"set14\", \"set15\", \"set16\",\"set17\",\"set18\",\"set19\",\"set20\"]\n",
    "#channel_names = [\"phase\", \"nuclear\"]\n",
    "channel_names = [\"phase\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify the number of features (cell edges and cell interior) that we have annotated. Furthermore, we need to identify which features are edges and whether we want to augment those edges with a dilation operation (https://en.wikipedia.org/wiki/Dilation_%28morphology%29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_features = 2\n",
    "is_edge_feature = [1,0]\n",
    "dil_radius = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to get a list of all the files in the training directories and initialize some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_direcs = len(training_direcs)\n",
    "num_channels = len(channel_names)\n",
    "\n",
    "imglist = []\n",
    "for direc in training_direcs:\n",
    "\timglist += os.listdir(os.path.join(direc_name, direc))\n",
    "\n",
    "# Load one file to get image sizes\n",
    "img_temp = get_image(os.path.join(direc_name,training_direcs[0],imglist[0]))\n",
    "image_size_x, image_size_y = img_temp.shape\n",
    "\n",
    "# Initialize arrays for the training images and the feature masks\n",
    "channels = np.zeros((num_direcs, num_channels, image_size_x, image_size_y), dtype='float32')\n",
    "feature_mask = np.zeros((num_direcs, num_of_features + 1, image_size_x, image_size_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block of code is responsible for loading all of the images, normalizing the training data, augmenting the edges by morphological dilation (if chosen) and constructing a feature map for the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training images\n",
    "direc_counter = 0\n",
    "for direc in training_direcs:\n",
    "\timglist = os.listdir(os.path.join(direc_name, direc))\n",
    "\tchannel_counter = 0\n",
    "\n",
    "\t# Load channels\n",
    "\tfor channel in channel_names:\n",
    "\t\tfor img in imglist: \n",
    "\t\t\tif fnmatch.fnmatch(img, r'*' + channel + r'*'):\n",
    "\t\t\t\tchannel_file = img\n",
    "\t\t\t\tchannel_file = os.path.join(direc_name, direc, channel_file)\n",
    "\t\t\t\tchannel_img = get_image(channel_file)\n",
    "\t\t\t\n",
    "                # Normalize the images\n",
    "\t\t\t\tp50 = np.percentile(channel_img, 50)\n",
    "\t\t\t\tchannel_img /= p50\n",
    "\n",
    "\t\t\t\tavg_kernel = np.ones((2*window_size_x + 1, 2*window_size_y + 1))\n",
    "\t\t\t\tchannel_img -= ndimage.convolve(channel_img, avg_kernel)/avg_kernel.size\n",
    "\n",
    "\t\t\t\tchannels[direc_counter,channel_counter,:,:] = channel_img\n",
    "\t\t\t\tchannel_counter += 1\n",
    "\n",
    "\t# Load feature mask\n",
    "\tfor j in xrange(num_of_features):\n",
    "\t\tfeature_name = \"feature_\" + str(j) + r\".*\"\n",
    "\t\tfor img in imglist:\n",
    "\t\t\tif fnmatch.fnmatch(img, feature_name):\n",
    "\t\t\t\tfeature_file = os.path.join(direc_name, direc, img)\n",
    "\t\t\t\tfeature_img = get_image(feature_file)\n",
    "\n",
    "\t\t\t\tif np.sum(feature_img) > 0:\n",
    "\t\t\t\t\tfeature_img /= np.amax(feature_img)\n",
    "\n",
    "\t\t\t\tif is_edge_feature[j] == 1:\n",
    "\t\t\t\t\tstrel = sk.morphology.disk(dil_radius)\n",
    "\t\t\t\t\tfeature_img = sk.morphology.binary_dilation(feature_img, selem = strel)\n",
    "\n",
    "\t\t\t\tfeature_mask[direc_counter,j,:,:] = feature_img\n",
    "\n",
    "\t# Thin the augmented edges by subtracting the interior features.\n",
    "\tfor j in xrange(num_of_features):\n",
    "\t\tif is_edge_feature[j] == 1:\n",
    "\t\t\tfor k in xrange(num_of_features):\n",
    "\t\t\t\tif is_edge_feature[k] == 0:\n",
    "\t\t\t\t\tfeature_mask[direc_counter,j,:,:] -= feature_mask[direc_counter,k,:,:]\n",
    "\t\t\tfeature_mask[direc_counter,j,:,:] = feature_mask[direc_counter,j,:,:] > 0\n",
    "\n",
    "\t# Compute the mask for the background\n",
    "\tfeature_mask_sum = np.sum(feature_mask[direc_counter,:,:,:], axis = 0)\n",
    "\tfeature_mask[direc_counter,num_of_features,:,:] = 1 - feature_mask_sum\n",
    "\n",
    "\tdirec_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training data to double check that it's loaded. If you're running the script on a server, this part may need to be commented out in the make_training_data.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(len(training_direcs),num_of_features+1, squeeze = False)\n",
    "for j in xrange(len(training_direcs)):\n",
    "\tax[j,0].imshow(channels[j,0,:,:],cmap=plt.cm.gray,interpolation='nearest')\n",
    "\tdef form_coord(x,y):\n",
    "\t\treturn cf(x,y,channels[j,0,:,:])\n",
    "\tax[j,0].format_coord = form_coord\n",
    "\tax[j,0].axes.get_xaxis().set_visible(False)\n",
    "\tax[j,0].axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\tfor k in xrange(1,num_of_features+1):\n",
    "\t\tax[j,k].imshow(feature_mask[j,k-1,:,:],cmap=plt.cm.gray,interpolation='nearest')\n",
    "\t\tdef form_coord(x,y):\n",
    "\t\t\treturn cf(x,y,feature_mask[j,k-1,:,:])\n",
    "\t\tax[j,k].format_coord = form_coord\n",
    "\t\tax[j,k].axes.get_xaxis().set_visible(False)\n",
    "\t\tax[j,k].axes.get_yaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've written this code to sample equally from each training image present. We first identify the feature that has the fewest examples (typically edges) and then we identify which training set has the smallest number of pixels in that feature category. We need to find the training data set with the least number of edge pixels. We then sample that number of pixels from each of the training data sets (if possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_mask_trimmed = feature_mask[:,:,window_size_x+1:-window_size_x-1,window_size_y+1:-window_size_y-1] \n",
    "feature_rows = []\n",
    "feature_cols = []\n",
    "feature_batch = []\n",
    "feature_label = []\n",
    "\n",
    "edge_num = np.Inf\n",
    "for j in xrange(feature_mask_trimmed.shape[0]):\n",
    "\tnum_of_edge_pixels = 0\n",
    "\tfor k in xrange(len(is_edge_feature)):\n",
    "\t\tif is_edge_feature[k] == 1:\n",
    "\t\t\tnum_of_edge_pixels += np.sum(feature_mask_trimmed[j,k,:,:])\n",
    "\n",
    "\tif num_of_edge_pixels < edge_num:\n",
    "\t\tedge_num = num_of_edge_pixels\n",
    "\n",
    "min_pixel_counter = edge_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know how many pixels to sample, we can identify which pixels to sample from the training data. The identity of each pixel requires 3 values - which row (feature_rows), which column (feature_cols) and which training image (feature_batch). The values of all 3 are saved as arrays with the corresponding names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for direc in xrange(channels.shape[0]):\n",
    "\n",
    "\tfor k in xrange(num_of_features + 1):\n",
    "\t\tfeature_counter = 0\n",
    "\t\tfeature_rows_temp, feature_cols_temp = np.where(feature_mask[direc,k,:,:] == 1)\n",
    "\n",
    "\t\t# Check to make sure the features are actually present\n",
    "\t\tif len(feature_rows_temp) > 0:\n",
    "\n",
    "\t\t\t# Randomly permute index vector\n",
    "\t\t\tnon_rand_ind = np.arange(len(feature_rows_temp))\n",
    "\t\t\trand_ind = np.random.choice(non_rand_ind, size = len(feature_rows_temp), replace = False)\n",
    "\n",
    "\t\t\tfor i in rand_ind:\n",
    "\t\t\t\tif feature_counter < min_pixel_counter:\n",
    "\t\t\t\t\tif (feature_rows_temp[i] - window_size_x > 0) and (feature_rows_temp[i] + window_size_x < image_size_x): \n",
    "\t\t\t\t\t\tif (feature_cols_temp[i] - window_size_y > 0) and (feature_cols_temp[i] + window_size_y < image_size_y):\n",
    "\t\t\t\t\t\t\tfeature_rows += [feature_rows_temp[i]]\n",
    "\t\t\t\t\t\t\tfeature_cols += [feature_cols_temp[i]]\n",
    "\t\t\t\t\t\t\tfeature_batch += [direc]\n",
    "\t\t\t\t\t\t\tfeature_label += [k]\n",
    "\t\t\t\t\t\t\tfeature_counter += 1\n",
    "\n",
    "feature_rows = np.array(feature_rows,dtype = 'int32')\n",
    "feature_cols = np.array(feature_cols,dtype = 'int32')\n",
    "feature_batch = np.array(feature_batch, dtype = 'int32')\n",
    "feature_label = np.array(feature_label, dtype = 'int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we clip the number of training examples so that it is less than the maximum number of allowed training examples. We also randomly shuffle the order of the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly select training points \n",
    "if len(feature_rows) > max_training_examples:\n",
    "\tnon_rand_ind = np.arange(len(feature_rows))\n",
    "\trand_ind = np.random.choice(non_rand_ind, size = max_training_examples, replace = False)\n",
    "\n",
    "\tfeature_rows = feature_rows[rand_ind]\n",
    "\tfeature_cols = feature_cols[rand_ind]\n",
    "\tfeature_batch = feature_batch[rand_ind]\n",
    "\tfeature_label = feature_label[rand_ind]\n",
    "\n",
    "# Randomize\n",
    "non_rand_ind = np.arange(len(feature_rows))\n",
    "rand_ind = np.random.choice(non_rand_ind, size = len(feature_rows), replace = False)\n",
    "\n",
    "feature_rows = feature_rows[rand_ind]\n",
    "feature_cols = feature_cols[rand_ind]\n",
    "feature_batch = feature_batch[rand_ind]\n",
    "feature_label = feature_label[rand_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets save the training data as an npz file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savez(file_name_save, channels = channels, y = feature_label, batch = feature_batch, pixels_x = feature_rows, pixels_y = feature_cols, win_x = window_size_x, win_y = window_size_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
